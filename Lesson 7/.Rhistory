}
all_k <- as.data.frame(all_k)
ggplot(all_k, aes(V1)) + geom_histogram()
mean(all_k$V1)
new_mixture_2 <- generate_mixture_data_trials(6, 100, 0.1, 100)
all_k_2 <- NULL
numtrials <- seq_len(100)
for (i in numtrials){
k_est_2 <- get_estimated_k(10, new_mixture_2[[i]])
all_k_2 <- rbind(all_k_2, k_est_2)
}
all_k_2 <- as.data.frame(all_k_2)
ggplot(all_k_2, aes(V1)) + geom_histogram()
mean(all_k_2$V1)
?saveRDS
?paste0
install.packages('magick')
img <- magick::image_read("./1_estimated_clusters.png")
plot(img)
#install.packages('magick')
library(magick)
img <- magick::image_read("./1_estimated_clusters.png")
plot(img)
estimated_k_o2 <- readRDS("./1_estimated_clusters.RDS")
print(estimated_k_o2)
a <- 1
b <- 2
c <- 3
d <- c(a, b, c)
d
dim(d)
as.data.frame(d)
d <- as.data.frame(variance = c(0.1, 0.2, 0.3), mean_k = c(a, b, c))
d <- data.frame(variance = c(0.1, 0.2, 0.3), mean_k = c(a, b, c))
d
knitr::opts_chunk$set(echo = TRUE)
if (!require(emojifont)) {
install.packages("emojifont")
library(emojifont)
}
library(tidyverse)
mean(readRDS("1_estimated_clusters.RDS"))
a <- readRDS("1_estimated_clusters.RDS")
a
mean(a$V1)
mean(a$V1)
mean(readRDS("1_estimated_clusters.RDS")$V1)
mean(readRDS("1_estimated_clusters.RDS")$V1)
?readrds
?readRDS
img <- magick::image_read("./result.png")
plot(img)
knitr::opts_chunk$set(echo = TRUE)
if (!require(emojifont)) {
install.packages("emojifont")
library(emojifont)
}
library(tidyverse)
# CODE 0.0.9000
df <- cars[1:5, ]
nrow(df)
install.packages("MASS")
library(MASS)
packageVersion("MASS")
library(MASS)
library(tidyverse)
generate_mixture_data <- function(number_of_components, component.size, variance){
# This represents the dimensionality of each data point
dimensions <- 2
# This represents the covariance matrix between each the dimensions of each cluster
variances <- diag(x = variance, nrow = dimensions, ncol = dimensions, names = FALSE)
all_components <- lapply(seq(1,number_of_components), function(component_i){ # Generate a set of components/clusters
means_component_i <- runif(n = dimensions, min=-10,max=10) # Randomly select the position of the component/cluster
normal_component_i <- mvrnorm(n = component.size, mu = means_component_i, Sigma = variances) # Sample from a normal distribution
return(normal_component_i)
})
mixture_data <- do.call("rbind", all_components) # Combine all the component data together in one large matrix
return(mixture_data)
}
plot(generate_mixture_data(3, 100, 1))
myvariances <- c(0.1, 0.5, 1, 5)
myvariances_mixtures <- lapply(myvariances, generate_mixture_data, number_of_components = 6, component.size = 100)
library(ggplot2)
library(dplyr)
myvariances_mixtures_df_1 <- as.data.frame(myvariances_mixtures[[1]])
myvariances_mixtures_df_1 <- myvariances_mixtures_df_1 %>% mutate(Cluster = "1")
myvariances_mixtures_df_2 <- as.data.frame(myvariances_mixtures[[2]])
myvariances_mixtures_df_2 <- myvariances_mixtures_df_2 %>% mutate(Cluster = "2")
myvariances_mixtures_df_3 <- as.data.frame(myvariances_mixtures[[3]])
myvariances_mixtures_df_3 <- myvariances_mixtures_df_3 %>% mutate(Cluster = "3")
myvariances_mixtures_df_4 <- as.data.frame(myvariances_mixtures[[4]])
myvariances_mixtures_df_4 <- myvariances_mixtures_df_4 %>% mutate(Cluster = "4")
myvariances_mixtures_df_plotready <- myvariances_mixtures_df_1 %>% full_join(myvariances_mixtures_df_2) %>%
full_join(myvariances_mixtures_df_3) %>%
full_join(myvariances_mixtures_df_4)
ggplot(myvariances_mixtures_df_plotready, aes(V1, V2)) +
geom_point() +
facet_wrap(vars(Cluster))
generate_mixture_data_trials <- function(number_of_components, component.size, variance, trials){
lapply(seq_len(trials), function(x) generate_mixture_data(number_of_components = number_of_components, component.size = component.size, variance = variance))
}
#lapply(seq_len(2), function(x) generate_mixture_data(number_of_components = 3, component.size = 3, variance = 0.1))
mixture_data_trials <- generate_mixture_data_trials(6, 100, 5, 10)
mixture_data_trials_1 <- as.data.frame(mixture_data_trials[[1]])
mixture_data_trials_1 <- mixture_data_trials_1 %>% mutate(Dataset = "1")
mixture_data_trials_2 <- as.data.frame(mixture_data_trials[[2]])
mixture_data_trials_2 <- mixture_data_trials_2 %>% mutate(Dataset = "2")
mixture_data_trials_3 <- as.data.frame(mixture_data_trials[[3]])
mixture_data_trials_3 <- mixture_data_trials_3 %>% mutate(Dataset = "3")
mixture_data_trials_4 <- as.data.frame(mixture_data_trials[[4]])
mixture_data_trials_4 <- mixture_data_trials_4 %>% mutate(Dataset = "4")
mixture_data_trials_5 <- as.data.frame(mixture_data_trials[[5]])
mixture_data_trials_5 <- mixture_data_trials_5 %>% mutate(Dataset = "5")
mixture_data_trials_6 <- as.data.frame(mixture_data_trials[[6]])
mixture_data_trials_6 <- mixture_data_trials_6 %>% mutate(Dataset = "6")
mixture_data_trials_7 <- as.data.frame(mixture_data_trials[[7]])
mixture_data_trials_7 <- mixture_data_trials_7 %>% mutate(Dataset = "7")
mixture_data_trials_8 <- as.data.frame(mixture_data_trials[[8]])
mixture_data_trials_8 <- mixture_data_trials_8 %>% mutate(Dataset = "8")
mixture_data_trials_9 <- as.data.frame(mixture_data_trials[[9]])
mixture_data_trials_9 <- mixture_data_trials_9 %>% mutate(Dataset = "9")
mixture_data_trials_10 <- as.data.frame(mixture_data_trials[[10]])
mixture_data_trials_10 <- mixture_data_trials_10 %>% mutate(Dataset = "10")
mixture_data_trials_plotready <- mixture_data_trials_1 %>% full_join(mixture_data_trials_2) %>%
full_join(mixture_data_trials_3) %>%
full_join(mixture_data_trials_4) %>%
full_join(mixture_data_trials_5) %>%
full_join(mixture_data_trials_6) %>%
full_join(mixture_data_trials_7) %>%
full_join(mixture_data_trials_8) %>%
full_join(mixture_data_trials_9) %>%
full_join(mixture_data_trials_10)
ggplot(mixture_data_trials_plotready, aes(V1, V2)) +
geom_point() +
facet_wrap(vars(Dataset))
library(cluster)
all_sil_scores <- NULL
get_estimated_k <- function(kmax, mixture_data){
k <- 2:kmax
for (k.i in k) {
kc <- kmeans(mixture_data, centers = k.i, nstart=25)
sil <- silhouette(kc$cluster, dist(mixture_data))
all_sil_scores <- rbind(all_sil_scores, mean(sil[, 3]))
}
k.estimated <- k[which(all_sil_scores == max(all_sil_scores))]
#return(all_sil_scores)
return(k.estimated)
}
#a <- get_estimated_k(10, mixture_data_trials[[1]])
#a
new_mixture <- generate_mixture_data_trials(6, 100, 1, 100)
#cluster_count <- lapply(seq_len(100), get_estimated_k, kmax = 10)
all_k <- NULL
numtrials <- seq_len(100)
for (i in numtrials){
k_est <- get_estimated_k(10, new_mixture[[i]])
all_k <- rbind(all_k, k_est)
}
all_k <- as.data.frame(all_k)
ggplot(all_k, aes(V1)) + geom_histogram()
mean(all_k$V1)
new_mixture_2 <- generate_mixture_data_trials(6, 100, 0.1, 100)
all_k_2 <- NULL
numtrials <- seq_len(100)
for (i in numtrials){
k_est_2 <- get_estimated_k(10, new_mixture_2[[i]])
all_k_2 <- rbind(all_k_2, k_est_2)
}
all_k_2 <- as.data.frame(all_k_2)
ggplot(all_k_2, aes(V1)) + geom_histogram()
mean(all_k_2$V1)
#install.packages('magick')
library(magick)
img <- magick::image_read("./1_estimated_clusters.png")
plot(img)
estimated_k_o2 <- readRDS("./1_estimated_clusters.RDS")
print(estimated_k_o2)
img <- magick::image_read("./result.png")
plot(img)
#Similar to other file, but here I split X into a test and train set of data, and compare how we did.
#Me trying to use the same logic
X<-read.csv("./CovidAllLabels.csv")
#I can't get data() to work on X because it is not a dataset?
#I made X a data table so some of the other functions would work
X<-as.data.table(X)
#Similar to other file, but here I split X into a test and train set of data, and compare how we did.
#Me trying to use the same logic
X<-read.csv("./CovidAllLabels.csv")
#I can't get data() to work on X because it is not a dataset?
#I made X a data table so some of the other functions would work
X<-data.table(X)
#Similar to other file, but here I split X into a test and train set of data, and compare how we did.
#Me trying to use the same logic
X<-read.csv("./CovidAllLabels.csv")
#I can't get data() to work on X because it is not a dataset?
#I made X a data table so some of the other functions would work
X<- as.data.table(X)
#Similar to other file, but here I split X into a test and train set of data, and compare how we did.
#Me trying to use the same logic
X<-read.csv("./CovidAllLabels.csv")
#I can't get data() to work on X because it is not a dataset?
#I made X a data table so some of the other functions would work
X<- as.data.frame(X)
#X$CC comes in as data type = factor, so had to change so tokenization worked
X$CC<-as.character(X$CC)
#changed X$patientID to characters since the identical function later requires this
X$patientID<-as.character(X$patientID)
setDT(X)
#Similar to other file, but here I split X into a test and train set of data, and compare how we did.
#Me trying to use the same logic
X<-read.csv("./CovidAllLabels.csv")
#I can't get data() to work on X because it is not a dataset?
#I made X a data table so some of the other functions would work
X<- as.data.frame(X)
#X$CC comes in as data type = factor, so had to change so tokenization worked
X$CC<-as.character(X$CC)
#changed X$patientID to characters since the identical function later requires this
X$patientID<-as.character(X$patientID)
#setDT(X)
setkey(X, patientID)
#Similar to other file, but here I split X into a test and train set of data, and compare how we did.
install.packages("text2vec", dependencies = TRUE)
install.packages("data.table")
install.packages("magrittr")
install.packages("glmnet")
install.packages("futile.options")
library(futile.options)
#Similar to other file, but here I split X into a test and train set of data, and compare how we did.
install.packages("text2vec", dependencies = TRUE)
install.packages("data.table")
install.packages("magrittr")
install.packages("glmnet")
install.packages("futile.options")
library(futile.options)
library(text2vec)
library(data.table)
library(magrittr)
library(glmnet)
#Me trying to use the same logic
X<-read.csv("./CovidAllLabels.csv")
#I can't get data() to work on X because it is not a dataset?
#I made X a data table so some of the other functions would work
X<- as.data.frame(X)
#X$CC comes in as data type = factor, so had to change so tokenization worked
X$CC<-as.character(X$CC)
#changed X$patientID to characters since the identical function later requires this
X$patientID<-as.character(X$patientID)
setDT(X)
setkey(X, patientID)
set.seed(2017L)
all_ids=X$patientID
train_ids = sample(all_ids, (length(all_ids)/2), replace=FALSE)
test_ids = setdiff(all_ids, train_ids)
#Build the train data set, J is join
train=X[J(train_ids)]
test = X[J(test_ids)]
# define preprocessing function and tokenization function
#install.packages("tokenizers")
#library(tokenizers)
#tolower converts everything into lower case
prep_fun = tolower
tok_fun = tokenize_lines
tok_fun = word_tokenizer
# define preprocessing function and tokenization function
#install.packages("tokenizers")
#library(tokenizers)
#tolower converts everything into lower case
prep_fun = tolower
tok_fun = word_tokenizer
it_train = itoken(train$CC,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = train$patientID,
progressbar = FALSE)
vocab = create_vocabulary(it_train)
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
#dtm train is a DTM Document Term Matrix
#check dimensions
dim(dtm_train)
identical(rownames(dtm_train), train$patientID)
NFOLDS = 4
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['Predicted']],
family = 'binomial',
# L1 penalty
alpha = 1,
# interested in the area under ROC curve
type.measure = "auc",
# 5-fold cross-validation
nfolds = NFOLDS,
# high value is less accurate, but has faster training
thresh = 1e-3,
# again lower number of iterations for faster training
maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))
#This output the AUC curve
plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
#Testing on train data, can basically replace w/ test where train is, this is just to validate
# Note that most text2vec functions are pipe friendly!
it_test = tok_fun(prep_fun(test$CC))
# turn off progressbar because it won't look nice in rmd
it_test = itoken(it_test, ids = test$patientID, progressbar = FALSE)
dtm_test = create_dtm(it_test, vectorizer)
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(test$Predicted, preds)
preds
data.frams(preds)
data.frame(preds)
dtm_test
data.frame(dtm_test)
data.frame(dtm_test)
it_test
class(it_test)
dtm_test
dim(dtm_test)
preds
data.frame(preds)
4666*2
saveRDS(data.frame(preds), "test_predictions.csv")
saveRDS(preds, "test_predictions.csv")
data.frame(preds)
write.csv(preds, "test_predictions")
write.csv(preds, "test_predictions.csv")
#this is the example data that was given, don't bother running this, it's just to help trouble shoot
install.packages("text2vec")
library(text2vec)
library(data.table)
library(magrittr)
library(tidyverse)
install.packages("text2vec")
install.packages("text2vec")
install.packages("text2vec")
#Me trying to use the same logic
X<-read.csv("./CovidAllLabels.csv")
#I can't get data() to work on X because it is not a dataset?
#I made X a data table so some of the other functions would work
X<-as.data.table(X)
#I can't get data() to work on X because it is not a dataset?
#I made X a data table so some of the other functions would work
X<-data.table(X)
install.packages("data.table")
install.packages("data.table")
library(data.table)
#Me trying to use the same logic
X<-read.csv("./CovidAllLabels.csv")
#I can't get data() to work on X because it is not a dataset?
#I made X a data table so some of the other functions would work
X <- as.data.table(X)
#X$CC comes in as data type = factor, so had to change so tokenization worked
X$CC<-as.character(X$CC)
#changed X$patientID to characters since the identical function later requires this
X$patientID<-as.character(X$patientID)
setDT(X)
setkey(X, patientID)
set.seed(2017L)
all_ids=X$patientID
#I am pretty sure we are training it with all phrases though I am not TOTALLY sure about that.  I don't know if we want it trained on existing values of 1/0 or our predicted values.
train_ids=all_ids
#since I am not sure what we are picking, there isn't anything in this yet
test_ids=setdiff(all_ids, train_ids)
#Build the train data set, J is join
train=X[J(train_ids)]
test = X[J(test_ids)]
# define preprocessing function and tokenization function
#install.packages("tokenizers")
library(tokenizers)
prep_fun = tolower
tok_fun = tokenize_lines
#the various defined terms in here are important, of note: I am using tokenize_lines as the tok_fun since it grabs the whole phrase, I donwloaded the tokenizers package to get more functionality
it_train = itoken(train$CC,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = train$patientID,
progressbar = FALSE)
# define preprocessing function and tokenization function
install.packages("tokenizers")
install.packages("tokenizers")
library(tokenizers)
?itoken
#this is the example data that was given, don't bother running this, it's just to help trouble shoot
#install.packages("text2vec")
#install.packages("data.table")
library(text2vec)
#Me trying to use the same logic
X<-read.csv("./CovidAllLabels.csv")
#I can't get data() to work on X because it is not a dataset?
#I made X a data table so some of the other functions would work
X <- as.data.table(X)
#X$CC comes in as data type = factor, so had to change so tokenization worked
X$CC<-as.character(X$CC)
#changed X$patientID to characters since the identical function later requires this
X$patientID<-as.character(X$patientID)
setDT(X)
setkey(X, patientID)
set.seed(2017L)
all_ids=X$patientID
#I am pretty sure we are training it with all phrases though I am not TOTALLY sure about that.  I don't know if we want it trained on existing values of 1/0 or our predicted values.
train_ids=all_ids
#since I am not sure what we are picking, there isn't anything in this yet
test_ids=setdiff(all_ids, train_ids)
#Build the train data set, J is join
train=X[J(train_ids)]
test = X[J(test_ids)]
# define preprocessing function and tokenization function
install.packages("tokenizers")
library(tokenizers)
prep_fun = tolower
tok_fun = tokenize_lines
#the various defined terms in here are important, of note: I am using tokenize_lines as the tok_fun since it grabs the whole phrase, I donwloaded the tokenizers package to get more functionality
it_train = itoken(train$CC,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = train$patientID,
progressbar = FALSE)
vocab = create_vocabulary(it_train)
#Now that we have a vocabulary, we can construct a document-term matrix.
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
#dtm train is a DTM Document Term Matrix
#check dimensions
dim(dtm_train)
identical(rownames(dtm_train), train$patientID)
train
install.packages("tokenizers")
#Me trying to use the same logic
X<-read.csv("./CovidAllLabels.csv")
#I can't get data() to work on X because it is not a dataset?
#I made X a data table so some of the other functions would work
X <- as.data.table(X)
#X$CC comes in as data type = factor, so had to change so tokenization worked
X$CC<-as.character(X$CC)
#changed X$patientID to characters since the identical function later requires this
X$patientID<-as.character(X$patientID)
setDT(X)
setkey(X, patientID)
set.seed(2017L)
all_ids=X$patientID
#I am pretty sure we are training it with all phrases though I am not TOTALLY sure about that.  I don't know if we want it trained on existing values of 1/0 or our predicted values.
train_ids=all_ids
#since I am not sure what we are picking, there isn't anything in this yet
test_ids=setdiff(all_ids, train_ids)
#Build the train data set, J is join
train=X[J(train_ids)]
test = X[J(test_ids)]
# define preprocessing function and tokenization function
#install.packages("tokenizers")
library(tokenizers)
prep_fun = tolower
tok_fun = tokenize_lines
#the various defined terms in here are important, of note: I am using tokenize_lines as the tok_fun since it grabs the whole phrase, I donwloaded the tokenizers package to get more functionality
it_train = itoken(train$CC,
preprocessor = prep_fun,
tokenizer = tok_fun,
ids = train$patientID,
progressbar = FALSE)
vocab = create_vocabulary(it_train)
#Now that we have a vocabulary, we can construct a document-term matrix.
vectorizer = vocab_vectorizer(vocab)
t1 = Sys.time()
dtm_train = create_dtm(it_train, vectorizer)
print(difftime(Sys.time(), t1, units = 'sec'))
#dtm train is a DTM Document Term Matrix
#check dimensions
dim(dtm_train)
identical(rownames(dtm_train), train$patientID)
train
#install.packages("glmnet")
library(glmnet)
#The train[[']] term is the parallel here to sentiment, either 'Result' or 'Predicted'.  Since there has to be a value in the column we are selecting, I used Predicted
NFOLDS = 4
t1 = Sys.time()
glmnet_classifier = cv.glmnet(x = dtm_train, y = train[['Predicted']],
family = 'binomial',
# L1 penalty
alpha = 1,
# interested in the area under ROC curve
type.measure = "auc",
# 5-fold cross-validation
nfolds = NFOLDS,
# high value is less accurate, but has faster training
thresh = 1e-3,
# again lower number of iterations for faster training
maxit = 1e3)
print(difftime(Sys.time(), t1, units = 'sec'))
#I don't know what these are
plot(glmnet_classifier)
print(paste("max AUC =", round(max(glmnet_classifier$cvm), 4)))
#Testing on train data, can basically replace w/ test where train is, this is just to validate
# Note that most text2vec functions are pipe friendly!
it_test = tok_fun(prep_fun(train$CC))
# turn off progressbar because it won't look nice in rmd
it_test = itoken(it_train, ids = train$patientID, progressbar = FALSE)
dtm_test = create_dtm(it_train, vectorizer)
preds = predict(glmnet_classifier, dtm_test, type = 'response')[,1]
glmnet:::auc(train$Predicted, preds)
preds
class(preds)
preds = predict(glmnet_classifier, dtm_test, type = 'response')#[,1]
preds
head(preds)
preds_full = predict(glmnet_classifier, dtm_train, type = 'response')
head(preds_full)
dtm_train
class(dtm_train)
dtm_test
dtm_train
